\documentclass{beamer}
\usepackage{amsmath,algorithm,algorithmic,graphicx,amsfonts,amsthm,color,pgf,tikz,wrapfig,amsfonts,multicol,wasysym,animate, appendixnumberbeamer}
\beamertemplatenavigationsymbolsempty
\useoutertheme[subsection=false]{miniframes}
\usetheme[progressbar=frametitle]{metropolis}
\metroset{block=fill}

\providecommand{\code}[1]{}
\renewcommand{\code}[1]{{\color{blue!80!black}\texttt{#1}}}
\def\hid#1#2{\onslide<#1>{#2}}


\title{Discrete Random Variables}
\author{David Gerard\\
Many slides borrowed from Linda Collins}
\date[\Sexpr{Sys.Date()}]{\Sexpr{Sys.Date()}}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
options(format.R.blank=FALSE)
options(width=60)
options(continue=" ")
options(replace.assign=TRUE)
options(scipen=8, digits=4)
opts_chunk$set(
  dev="pdf",
  fig.align='center',
  fig.width=7,
  fig.height=4,
  fig.pos='H',
  fig.show='asis',
  out.width='0.99\\linewidth',
  par=TRUE,
  tidy=FALSE,
  tidy.opts=list(width.cutoff=50),
  prompt=FALSE,
  comment=NA
)
@

\begin{frame}[fragile]
  \titlepage
\end{frame}

\begin{frame}{Learning Objectives}
\begin{itemize}
\item Random Variables
\item Discrete random variables.
\item Means of discrete random variables.
\item Means of functions of random variables.
\item Variances of discrete random variables.
\item Sections 2.4, 3.3.1, 3.4 in DBC
\end{itemize}
\end{frame}

\begin{frame}{Random Variable}
\begin{itemize}
\item Sequence of head/tails in 4 tosses. Possible events: HHHH, HTHT, HHTT, $\ldots$.
\item In a clinical trial, the list of all people who had side effects: Bob, Lisa, Drake, $\ldots$.
\item We are usually interested in summary level data, like the number of heads in a toss or the number of people who had side effects.
\end{itemize}
\begin{block}{random variable}
A \alert{random variable} is a variable whose value is a numerical outcome of a random process.

More formally, a \alert{random variable} is a function from the sample space to the real numbers $\mathbb{R}$.
\end{block}
\end{frame}

\begin{frame}{Example}
\begin{itemize}
\item Suppose $X$ is the number of heads on 4 flips of a fair coin. The domain of $X$ is $\{HHHH, HHHT, HHTH,\ldots, TTTT\}$.
\item The range of $X$ is $\{0, 1, 2, 3, 4\}$. This is also the sample space of $X$.
\item $X(HHHH) = $ \\
$X(HTTH) = $\\
$X(HTHT) = $
\end{itemize}
\end{frame}

\begin{frame}{Distribution}
\begin{itemize}
\item We are interested in the \alert{distribution} of a random variable ---  what values it can take and how often it takes those values.
\item What is the probability of 0 heads in 4 tosses? 3 heads in 4 tosses?
\item A random variable can have either a discrete or a continuous distribution.
\end{itemize}
\end{frame}

\begin{frame}{Discrete Random Variable}
\begin{block}{Discrete Random Variable}
A \alert{discrete random variable} $X$ has possible values that can be given in an ordered list. The \alert{probability distribution} of $X$ lists the values and their probabilities:
\begin{center}
\begin{tabular}{l|cccc}
\hline
Value of $X$ & $x_1$ & $x_2$ & $x_3$ & $\cdots$\\
\hline
Probability & $p_1$ & $p_2$ & $p_3$ & $\cdots$\\
\hline
\end{tabular}
\end{center}
The probabilities $p_i$ must satisfy two requirements:
\begin{enumerate}
\item Every probability $p_i$ is a number between 0 and 1.
\item $p_1 + p_2 + \cdots = 1$
\end{enumerate}
Find the probability of an event by adding the probabilities $p_i$ of the particular values $x_i$ that make up the event.
\end{block}
\end{frame}

\begin{frame}{pmf}
\begin{block}{pmf}
The \alert{probability mass function} (pmf) of a random variable maps the outcomes to their individual probabilities.
\end{block}
So $P(x_i) = p_i$, where
\begin{center}
\begin{tabular}{l|cccc}
\hline
Value of $X$ & $x_1$ & $x_2$ & $x_3$ & $\cdots$\\
\hline
Probability & $p_1$ & $p_2$ & $p_3$ & $\cdots$\\
\hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}{Coin example}
Let $X = $ number of heads in 4 tosses of a fair coin.\\
\begin{align*}
P(X = 1) &= P(\{HTTT, THTT, TTHT, TTTH\})\\
&= P(HTTT) + P(THTT) + P(TTHT) + P(TTTH)\\
&= 1/16 + 1/16 + 1/16 + 1/16\\
&= 4/16 = 1/4.
\end{align*}
\end{frame}

\begin{frame}{pmf for $X$}
We can summarize the pmf for this $X$ in this table
\begin{center}
\begin{tabular}{c|ccccc}
\hline
$x$ & 0 & 1 & 2 & 3 & 4\\
\hline
$f(x)$ & 1/16 & 4/16 & 6/16 & 4/16 & 1/16\\
\hline
\end{tabular}
\end{center}
Note: these probabilities sum to 1:
\begin{align*}
\sum_x f(x) = 1/16 + 4/16 + 6/16 + 4/16 + 1/16 = 1
\end{align*}
\end{frame}

\begin{frame}[fragile]{Probability Distribution for Discrete Random Variable}
\vspace{0.1cm}
Can find probabilities of events using pmf:
\vskip0.1cm
\begin{center}
\begin{tabular}{r|ccccc}
$x$ & 0&1&2&3&4\\
\hline
$f(x)$ & 1/16 &4/16&6/16&4/16&1/16\\
\end{tabular}
\end{center}
\vspace{-0.001cm}

$P(A) = P(X \in A)$
\vspace{0.05cm}
\begin{align*}
A&=\hbox{Event of 3 heads}\\
P(A) &=P(X=3) = 4/16\\[+0.35cm]
B&=\hbox{Event that all 4 flips result in either all heads or all tails}\\
P(B) &=P(X=0\cup X=4)=P(X=0)+P(X=4)=\frac{1}{16}+\frac{1}{16}=\frac{2}{16}\\[+0.35cm]
C&=\hbox{Event that at most one of the 4 flips is heads}\\
P(C) &=P(X\ge 3) = P(X=3 \cup X=4) =4/16+1/16=5/16\\
\end{align*}

\end{frame}

% \begin{frame}{Benford's Law}
% \begin{itemize}
% \item The first digits of numbers in legitimate financial records often follows \alert{Benford's law}.
% \end{itemize}
% \begin{center}
% \footnotesize
% \begin{tabular}{l|ccccccccc}
% \hline
% 1st Digit & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
% \hline
% Prob & 0.30 & 0.18 & 0.13 & 0.1 & 0.08 & 0.07 & 0.06 & 0.05 & 0.05\\
% \hline
% \end{tabular}
% \normalsize
% \end{center}
% \begin{itemize}
% \item Extreme deviations from these probabilities can alert investigators to fraud.
% \end{itemize}
% \end{frame}

\begin{frame}{The Mean}
The same way we try to describe the center and spread of data, we often want to describe the center and spread of the distribution of a random variable.

\begin{block}{Mean (Expected Value)}
Suppose $X$ is a discrete random variable, then
\begin{align*}
  \text{\alert{mean} of } X &= E[X] \\
  &= \sum_{\text{all } x}xP(X = x)\\
  &=\sum_{\text{all } x}xf(x)\\
  &=\mu
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Mean Intuition I: Benford's Law}
\begin{itemize}
\item The first digits of numbers in legitimate financial records often follows \alert{Benford's law}.
\end{itemize}
\begin{center}
\footnotesize
\begin{tabular}{l|ccccccccc}
\hline
1st Digit & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
\hline
Prob & 0.30 & 0.18 & 0.13 & 0.1 & 0.08 & 0.07 & 0.06 & 0.05 & 0.05\\
\hline
\end{tabular}
\normalsize
\end{center}
\begin{itemize}
\item Extreme deviations from these probabilities can alert investigators to fraud.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Mean Intuition II: Calculate Mean}
<<echo=FALSE>>=
ben <- c(0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046)
eben <- sum(1:9 * ben)
@
\begin{align*}
E[X] &= 1\times 0.30 + 2\times 0.18 + 3\times 0.13 + 4\times 0.1\\
& + 5\times 0.08 + 6\times 0.07 + 7\times 0.06 + 8\times 0.05 + 9\times 0.05\\
&= \Sexpr{eben}
\end{align*}
\end{frame}

\begin{frame}[fragile]{Mean Intuition III: Probability Histogram and Center of Mass}
<<echo=FALSE>>=
plot(ben, type = "h", ylab = "P(X = x)", xlab = "x", main = "Probability Histogram", lwd = 7, col = "grey50")
abline(v = eben, lty = 2, col = 2, lwd = 2)
legend("topright", lty = 2, col = 2, "Mean", lwd = 2)
@
\end{frame}

\begin{frame}{Mean Intuition IV: Average when simulate a lot of data}
Suppose we chose 100 digits at random using the distribution from Benford's Law

Then we would expect about
30 1's, 18 2's, 13 3's, etc...

If we take an average of the values we expect, we get:

\begin{align*}
&\frac{30\times 1 + 18\times 2 + 13\times 3 + \cdots + 5 \times 9}{100}\\
&= \frac{30}{100}\times 1 + \frac{18}{100}\times 2 + \frac{13}{100}\times 3 + \cdots + \frac{5}{100}\times 9\\
&= 0.3 \times 1 + 0.18 \times 1 + 0.13 \times 1 + \cdots + 0.05 \times 9\\
&= \Sexpr{eben}.
\end{align*}

Notice that we empirically derived the formula for expected value that we used earlier.
\end{frame}

\begin{frame}[fragile]{Mean Intuition V: Average when simulate a lot of data}
Recall: most samples of 100 digits won't be exactly equal to \Sexpr{eben}. But if we took a sample of MANY MANY digits, we could get pretty close:
<<>>=
ben <- c(0.301, 0.176, 0.125, 0.097,
         0.079, 0.067, 0.058, 0.051, 0.046)
sample_digits <- sample(x = 1:9, size = 10000,
                        prob = ben, replace = TRUE)
mean(sample_digits)
@
Recall that the mean is \Sexpr{eben}
\end{frame}

\begin{frame}[fragile]{Mean Intuition VI: Average when simulate a lot of data}
<<echo = FALSE>>=
mean_vec <- cumsum(sample_digits) / 1:length(sample_digits)
plot(mean_vec, type = "l", xlab = "Number of Observations", ylab = "Sample Mean")
abline(h = eben, lty = 2, col = 2, lwd = 2)
legend("topright", "True Mean", lty = 2, lwd = 2, col = 2)
@
So $E[X] = $ average when we take a HUGE sample.
\end{frame}

\begin{frame}[allowframebreaks, fragile]{Functions of a random variable}

If $X: S \rightarrow \mathbb{R}$ is a random variable, and $g:\mathbb{R} \rightarrow \mathbb{R}$ is a function
then $Y=g(X)$ is also a random variable with range $R_Y=g(R_X)$.

Example: For Benford's law example take
$g(x)=\begin{cases} 1 & \mbox{if} \ x \leq 5 \\ 0 & \mbox{if} \ x > 5 \end{cases}$.

Then
$Y=g(X)$ has range $R_Y=\{1, 0\}$ and distribution $f_Y(1)=P(Y=1)=P(X\in\{1, 2, 3, 4, 5\})=\Sexpr{sum(ben[1:5])}$
$f_Y(0)=P(Y=0)=P(X\in\{6, 7, 8, 9\})=\Sexpr{sum(ben[6:9])}.$

So we can compute the mean: $E(Y) = 1 \cdot \Sexpr{sum(ben[1:5])} + 0 \cdot \Sexpr{sum(ben[6:9])}= \Sexpr{sum(ben[1:5])}$

\medskip
Sometimes it is difficult to compute the new distribution of $Y$.

It's still easy to compute $E(Y)$ in terms of the distribution of $X$.
\end{frame}

\begin{frame}[fragile]{Functions of random variables ii}
\begin{block}{Law of \alert{unconscious statistican}}
Suppose we know the pmf of $X$ and $Y = g(X)$, then
\begin{align*}
E(Y) = E g(X) = \sum_{k=1}^K g(x_k) f_X(x_k).
\end{align*}
\end{block}
\end{frame}

\begin{frame}[fragile]{Functions of random variables iii}

\footnotesize
\begin{tabular}{l|ccccccccc}
\hline
1st Digit & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
\hline
Prob & 0.30 & 0.18 & 0.13 & 0.1 & 0.08 & 0.07 & 0.06 & 0.05 & 0.05\\
\hline
\end{tabular}
\normalsize

So in our Benford's law example we compute
\begin{align*}
E(Y) =& g(1) \cdot \Sexpr{ben[1]} + g(2) \cdot \Sexpr{ben[2]} + g(3) \cdot \Sexpr{ben[3]} \\
      &+ g(4) \cdot \Sexpr{ben[4]} + g(5) \cdot \Sexpr{ben[5]} + g(6) \cdot \Sexpr{ben[6]} \\
      &+ g(7) \cdot \Sexpr{ben[7]} + g(8) \cdot \Sexpr{ben[8]} + g(9) \cdot \Sexpr{ben[9]}\\
     =& 1 \cdot \Sexpr{ben[1]} + 1 \cdot \Sexpr{ben[2]} + 1 \cdot \Sexpr{ben[3]} \\
      &+ 1 \cdot \Sexpr{ben[4]} + 1 \cdot \Sexpr{ben[5]} + 0 \cdot \Sexpr{ben[6]} \\
      &+ 0 \cdot \Sexpr{ben[7]} + 0 \cdot \Sexpr{ben[8]} + 0 \cdot \Sexpr{ben[9]}\\
     =& \Sexpr{sum(ben[1:5])}.
\end{align*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Parameters: Describing the Spread of a r.v.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One measure of spread a probability distribution is {\bf variance}.
\vskip0.2cm

\textbf{Variance:}
The variance of a probability distribution \\
is the \underline{average squared distance from the mean}.

\begin{block}{Variance}
$$
\text{\alert{Variance} of\ } X
= Var(X)
= \sigma^2
= \text{\ ``sigma squared"\ }
= E[(X-\mu)^2]
$$
\end{block}

Variance is in squared units.  \\
Take the square root to determine the {\bf standard deviation.}
$$
\mbox{Standard Deviation of\ } X
= \sqrt{Var(X)}
= \sigma
= \mbox{\ ``sigma"\ }
= SD(X)
$$
One standard deviation is roughly \\
the ``typical" distance of outcomes from the mean.
\vskip0.2cm

\end{frame}
%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%
\begin{frame}{A Formula Useful for Calculating Variance of a r.v.}

This formula is not intuitive!  Instead, think of variance as \\
``the average squared distance of outcomes from their mean."
\vskip0.2cm

However, to make calculation of variance easier, we can show that
$$Var(X) = E[(X-\mu)^2] = E(X^2) - (\mu)^2$$
\begin{align*}
E[(X-\mu)^2]
&= \sum_x (x-\mu)^2\; f(x)\\
&= \hid{2-}{\sum_x (x^2-2\mu x+\mu^2)\; f(x)}\\
&= \underbrace{\hid{3-}{\sum_x x^2\; f(x)}}_{\displaystyle \hid{4-}{=E(X^2)}}
   \hid{3-}{-2\mu} \underbrace{\hid{3-}{\sum_x x\; f(x)}}_{\displaystyle \hid{5-}{=\mu} }
   \hid{3-}{+\mu^2} \underbrace{\hid{3-}{\sum_x f(x)}}_{\displaystyle \hid{6-}{=1}}\\
&= \hid{7-}{E(X^2) - 2\mu^2 + \mu^2}
\hskip1cm
 = \hid{8-}{E(X^2) - \mu^2}
\end{align*}

\end{frame}


\end{document}
