\documentclass{beamer}
\usepackage{amsmath,algorithm,algorithmic,graphicx,amsfonts,amsthm,color,pgf,tikz,wrapfig,amsfonts,multicol,wasysym,animate, appendixnumberbeamer}
\beamertemplatenavigationsymbolsempty
\useoutertheme[subsection=false]{miniframes}
\usetheme[progressbar=frametitle]{metropolis}
\metroset{block=fill}

\providecommand{\code}[1]{}
\renewcommand{\code}[1]{{\color{blue!80!black}\texttt{#1}}}
\def\hid#1#2{\onslide<#1>{#2}}


\title{Discrete Random Variables}
\author{David Gerard\\
Many slides borrowed from Linda Collins}
\date[\Sexpr{Sys.Date()}]{\Sexpr{Sys.Date()}}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
options(format.R.blank=FALSE)
options(width=60)
options(continue=" ")
options(replace.assign=TRUE)
options(scipen=8, digits=4)
opts_chunk$set(
  dev="pdf",
  fig.align='center',
  fig.width=7,
  fig.height=4,
  fig.pos='H',
  fig.show='asis',
  out.width='0.99\\linewidth',
  par=TRUE,
  tidy=FALSE,
  tidy.opts=list(width.cutoff=50),
  prompt=FALSE,
  comment=NA
)
@

\begin{frame}[fragile]
  \titlepage
\end{frame}

\begin{frame}{Learning Objectives}
\begin{itemize}
\item Random Variables
\item Discrete random variables.
\item Means of discrete random variables.
\item Variances of discrete random variables.
\item Bernoulli Distribution
\item Binomial Distribution
\item Sections 3.2, 3.3, 3.5, in Matoff
\item Sections 3.3.1, 3.4 in DBC
\end{itemize}
\end{frame}

\begin{frame}{Random Variable}
\begin{itemize}
\item Sequence of head/tails in 4 tosses. Possible events: HHHH, HTHT, HHTT, $\ldots$.
\item In a clinical trial, the list of all people who had side effects: Bob, Lisa, Drake, $\ldots$.
\item We are usually interested in summary level data, like the number of heads in a toss or the number of people who had side effects.
\end{itemize}
\begin{block}{random variable}
A \alert{random variable} is a variable whose value is a numerical outcome of a random process.

More formally, a \alert{random variable} is a function from the sample space to the real numbers $\mathbb{R}$.
\end{block}
\end{frame}

\begin{frame}{Example}
\begin{itemize}
\item Suppose $X$ is the number of heads on 4 flips of a fair coin. The domain of $X$ is $\{HHHH, HHHT, HHTH,\ldots, TTTT\}$.
\item The range of $X$ is $\{0, 1, 2, 3, 4\}$. This is also the sample space of $X$.
\item $X(HHHH) = $ \\
$X(HTTH) = $\\
$X(HTHT) = $
\end{itemize}
\end{frame}

\begin{frame}{Distribution}
\begin{itemize}
\item We are interested in the \alert{distribution} of a random variable ---  what values it can take and how often it takes those values.
\item What is the probability of 0 heads in 4 tosses? 3 heads in 4 tosses?
\item A random variable can have either a discrete or a continuous distribution.
\end{itemize}
\end{frame}

\begin{frame}{Discrete Random Variable}
\begin{block}{Discrete Random Variable}
A \alert{discrete random variable} $X$ has possible values that can be given in an ordered list. The \alert{probability distribution} of $X$ lists the values and their probabilities:
\begin{center}
\begin{tabular}{l|cccc}
\hline
Value of $X$ & $x_1$ & $x_2$ & $x_3$ & $\cdots$\\
\hline
Probability & $p_1$ & $p_2$ & $p_3$ & $\cdots$\\
\hline
\end{tabular}
\end{center}
The probabilities $p_i$ must satisfy two requirements:
\begin{enumerate}
\item Every probability $p_i$ is a number between 0 and 1.
\item $p_1 + p_2 + \cdots = 1$
\end{enumerate}
Find the probability of an event by adding the probabilities $p_i$ of the particular values $x_i$ that make up the event.
\end{block}
\end{frame}

\begin{frame}{pmf}
\begin{block}{pmf}
The \alert{probability mass function} (pmf) of a random variable maps the outcomes to their individual probabilities.
\end{block}
So $P(x_i) = p_i$, where
\begin{center}
\begin{tabular}{l|cccc}
\hline
Value of $X$ & $x_1$ & $x_2$ & $x_3$ & $\cdots$\\
\hline
Probability & $p_1$ & $p_2$ & $p_3$ & $\cdots$\\
\hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}{Coin example}
Let $X = $ number of heads in 4 tosses of a fair coin.\\
\begin{align*}
P(X = 1) &= P(\{HTTT, THTT, TTHT, TTTH\})\\
&= P(HTTT) + P(THTT) + P(TTHT) + P(TTTH)\\
&= 1/16 + 1/16 + 1/16 + 1/16\\
&= 4/16 = 1/4.
\end{align*}
\end{frame}

\begin{frame}{pmf for $X$}
We can summarize the pmf for this $X$ in this table
\begin{center}
\begin{tabular}{c|ccccc}
\hline
$x$ & 0 & 1 & 2 & 3 & 4\\
\hline
$f(x)$ & 1/16 & 4/16 & 6/16 & 4/16 & 1/16\\
\hline
\end{tabular}
\end{center}
Note: these probabilities sum to 1:
\begin{align*}
\sum_x f(x) = 1/16 + 4/16 + 6/16 + 4/16 + 1/16 = 1
\end{align*}
\end{frame}

\begin{frame}[fragile]{Probability Distribution for Discrete Random Variable}
\vspace{0.1cm}
Can find probabilities of events using pmf:
\vskip0.1cm
\begin{center}
\begin{tabular}{r|ccccc}
$x$ & 0&1&2&3&4\\
\hline
$f(x)$ & 1/16 &4/16&6/16&4/16&1/16\\
\end{tabular}
\end{center}
\vspace{-0.001cm}

$P(A) = P(X \in A)$
\vspace{0.05cm}
\begin{align*}
A&=\hbox{Event of 3 heads}\\
P(A) &=P(X=3) = 4/16\\[+0.35cm]
B&=\hbox{Event that all 4 flips result in either all heads or all tails}\\
P(B) &=P(X=0\cup X=4)=P(X=0)+P(X=4)=\frac{1}{16}+\frac{1}{16}=\frac{2}{16}\\[+0.35cm]
C&=\hbox{Event that at most one of the 4 flips is heads}\\
P(C) &=P(X\ge 3) = P(X=3 \cup X=4) =4/16+1/16=5/16\\
\end{align*}

\end{frame}

% \begin{frame}{Benford's Law}
% \begin{itemize}
% \item The first digits of numbers in legitimate financial records often follows \alert{Benford's law}.
% \end{itemize}
% \begin{center}
% \footnotesize
% \begin{tabular}{l|ccccccccc}
% \hline
% 1st Digit & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
% \hline
% Prob & 0.30 & 0.18 & 0.13 & 0.1 & 0.08 & 0.07 & 0.06 & 0.05 & 0.05\\
% \hline
% \end{tabular}
% \normalsize
% \end{center}
% \begin{itemize}
% \item Extreme deviations from these probabilities can alert investigators to fraud.
% \end{itemize}
% \end{frame}

\begin{frame}{The Mean}
The same way we try to describe the center and spread of data, we often want to describe the center and spread of the distribution of a random variable.

\begin{block}{Mean (Expected Value)}
Suppose $X$ is a discrete random variable, then
\begin{align*}
  \text{\alert{mean} of } X &= E[X] \\
  &= \sum_{\text{all } x}xP(X = x)\\
  &=\sum_{\text{all } x}xf(x)\\
  &=\mu
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Mean Intuition I: Benford's Law}
\begin{itemize}
\item The first digits of numbers in legitimate financial records often follows \alert{Benford's law}.
\end{itemize}
\begin{center}
\footnotesize
\begin{tabular}{l|ccccccccc}
\hline
1st Digit & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
\hline
Prob & 0.30 & 0.18 & 0.13 & 0.1 & 0.08 & 0.07 & 0.06 & 0.05 & 0.05\\
\hline
\end{tabular}
\normalsize
\end{center}
\begin{itemize}
\item Extreme deviations from these probabilities can alert investigators to fraud.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Mean Intuition II: Calculate Mean}
<<echo=FALSE>>=
ben <- c(0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046)
eben <- sum(1:9 * ben)
@
\begin{align*}
E[X] &= 1\times 0.30 + 2\times 0.18 + 3\times 0.13 + 4\times 0.1\\
& + 5\times 0.08 + 6\times 0.07 + 7\times 0.06 + 8\times 0.05 + 9\times 0.05\\
&= \Sexpr{eben}
\end{align*}
\end{frame}

\begin{frame}[fragile]{Mean Intuition III: Probability Histogram and Center of Mass}
<<echo=FALSE>>=
plot(ben, type = "h", ylab = "P(X = x)", xlab = "x", main = "Probability Histogram", lwd = 7, col = "grey50")
abline(v = eben, lty = 2, col = 2, lwd = 2)
legend("topright", lty = 2, col = 2, "Mean", lwd = 2)
@
\end{frame}

\begin{frame}{Mean Intuition IV: Average when simulate a lot of data}
Suppose we chose 100 digits at random using the distribution from Benford's Law

Then we would expect about
30 1's, 18 2's, 13 3's, etc...

If we take an average of the values we expect, we get:

\begin{align*}
&\frac{30\times 1 + 18\times 2 + 13\times 3 + \cdots + 5 \times 9}{100}\\
&= \frac{30}{100}\times 1 + \frac{18}{100}\times 2 + \frac{13}{100}\times 3 + \cdots + \frac{5}{100}\times 9\\
&= 0.3 \times 1 + 0.18 \times 1 + 0.13 \times 1 + \cdots + 0.05 \times 9\\
&= \Sexpr{eben}.
\end{align*}

Notice that we empirically derived the formula for expected value that we used earlier.
\end{frame}

\begin{frame}[fragile]{Mean Intuition V: Average when simulate a lot of data}
Recall: most samples of 100 digits won't be exactly equal to \Sexpr{eben}. But if we took a sample of MANY MANY digits, we could get pretty close:
<<>>=
ben <- c(0.301, 0.176, 0.125, 0.097,
         0.079, 0.067, 0.058, 0.051, 0.046)
sample_digits <- sample(x = 1:9, size = 10000,
                        prob = ben, replace = TRUE)
mean(sample_digits)
@
Recall that the mean is \Sexpr{eben}
\end{frame}

\begin{frame}[fragile]{Mean Intuition VI: Average when simulate a lot of data}
<<echo = FALSE>>=
mean_vec <- cumsum(sample_digits) / 1:length(sample_digits)
plot(mean_vec, type = "l", xlab = "Number of Observations", ylab = "Sample Mean")
abline(h = eben, lty = 2, col = 2, lwd = 2)
legend("topright", "True Mean", lty = 2, lwd = 2, col = 2)
@
So $E[X] = $ average when we take a HUGE sample.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Parameters: Describing the Spread of a r.v.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One measure of spread a probability distribution is {\bf variance}.
\vskip0.2cm

\textbf{Variance:}
The variance of a probability distribution \\
is the \underline{average squared distance from the mean}.

\begin{block}{Variance}
$$
\text{\alert{Variance} of\ } X
= Var(X)
= \sigma^2
= \text{\ ``sigma squared"\ }
= E[(X-\mu)^2]
$$
\end{block}

Variance is in squared units.  \\
Take the square root to determine the {\bf standard deviation.}
$$
\mbox{Standard Deviation of\ } X
= \sqrt{Var(X)}
= \sigma
= \mbox{\ ``sigma"\ }
= SD(X)
$$
One standard deviation is roughly \\
the ``typical" distance of outcomes from the mean.
\vskip0.2cm

\end{frame}
%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%
\begin{frame}{A Formula Useful for Calculating Variance of a r.v.}

This formula is not intuitive!  Instead, think of variance as \\
``the average squared distance of outcomes from their mean."
\vskip0.2cm

However, to make calculation of variance easier, we can show that
$$Var(X) = E[(X-\mu)^2] = E(X^2) - (\mu)^2$$
\begin{align*}
E[(X-\mu)^2]
&= \sum_x (x-\mu)^2\; f(x)\\
&= \hid{2-}{\sum_x (x^2-2\mu x+\mu^2)\; f(x)}\\
&= \underbrace{\hid{3-}{\sum_x x^2\; f(x)}}_{\displaystyle \hid{4-}{=E(X^2)}}
   \hid{3-}{-2\mu} \underbrace{\hid{3-}{\sum_x x\; f(x)}}_{\displaystyle \hid{5-}{=\mu} }
   \hid{3-}{+\mu^2} \underbrace{\hid{3-}{\sum_x f(x)}}_{\displaystyle \hid{6-}{=1}}\\
&= \hid{7-}{E(X^2) - 2\mu^2 + \mu^2}
\hskip1cm
 = \hid{8-}{E(X^2) - \mu^2}
\end{align*}

\end{frame}

\section{Deriving the Binomial PMF}

\begin{frame}{A formula}
\begin{itemize}
\item Sometimes, if you are lucky, the pmf may be written as an equation in terms of the value of the random variable.
\item For the coin flipping example, we will derive this formula.
\item Useful beyond just coins: What is the probabiliy of having 3 girls out of 4 children? I.e. many random variables \emph{follow the same distribution}.
\end{itemize}
\end{frame}






%-------------------------------------------------------
%-------------------------------------------------------
\begin{frame}[allowframebreaks]{Using counting rules to determine probabilities}
\vspace{0.5cm}

Since outcomes in the random flip are equally likely,
we just counted the outcomes to determine event probabilities.
\vskip0.5cm

Let's generalize the counting process for this probability model.
\vskip0.5cm

We want a formula for the number of outcomes \\
having $k$ heads out of $4$ flips.
\vskip0.5cm

We begin with a discussion of \\
\textbf{permutations} and\\
\textbf{combinations}
\dots also called \textbf{binomial coefficients}
\end{frame}

\begin{frame}{Permutations}

\textbf{Permutations:} \\~\\
How many ways to order a group of 4 people?
\begin{equation*}
4 \text{ choices for $1$st person } \times (3 \text{ for $2$nd }) \times (2 \text{ for $3$rd }) \times (1 \text{ for $4$th.})
\end{equation*}
How many ways to order a group of $n$ people?
\begin{equation*}
n \times (n-1) \times (n-2) \times \dots \times 2 \times 1 =n!
\end{equation*}
(Note: $n!$ is pronounced ``n factorial.'')
\end{frame}


\begin{frame}{Combinations}

\textbf{Combinations}, also called \textbf{Binomial Coefficients:}
\vskip0.2cm
(Let $n=5$ for the moment just for this one-slide example.)
\vskip0.2cm
How many ways to choose a committee of 2 from a group of 5?
\begin{align*}
& \frac{5 \text{ choices for 1st committee member } \times 4 \text{ for 2nd }}{2! \text{ orderings of $2$ person committee}} \\
&=
\frac{5 \cdot 4 \cdot (3 \cdot 2 \cdot 1)}{2!\; (3 \cdot 2 \cdot 1)}
= \frac{5!}{2!\; 3!}
\end{align*}
How many ways to choose a committee of $k$ from a group of $n$?
\begin{equation*}
{n \choose k} = \frac{n!}{k!\; (n-k)!}
\end{equation*}
Note: ${n \choose k}$ is pronounced ``$n$ choose $k$.''

\end{frame}

\begin{frame}{Counting outcomes}
\textbf{Counting outcomes} for 4 flips of a coin:
\vskip0.4cm

How many outcomes with $2$ heads out of $4$ flips?
\vskip0.4cm

Each sequence has four flips:
$\{\text{First}, \text{Second}, \text{Third}, \text{Fourth}\}$.
\vskip0.4cm

How many outcomes have two heads
\vskip0.2cm

That is, now many ways can we choose two locations from four:
$\{\text{First}, \text{Second}, \text{Third}, \text{Fourth}\}$?
\vskip0.6cm

There are $\displaystyle {n \choose k} = {4 \choose 2}$ ways!

\end{frame}

\begin{frame}{Verify}
\begin{itemize}
\item We can verify this directly\medskip
\item HHTT, HTHT, HTTH, THHT, THTH, TTHH\medskip
\item But this formula always works without having to directly count outcomes.
\end{itemize}
\end{frame}

\begin{frame}{pmf}
How many outcomes total?
\begin{equation*}
2 \text{ choices for $1$st } \times (2 \text{ for $2$nd }) \times (2 \text{ for $3$rd }) \times (2 \text{ for $4$th }) = 2^4 = 16.
\end{equation*}

\textbf{Probability mass function for $X$:}
\begin{equation*}
f(x) = \mathbf{P}(X=x) = \frac{\# \text{outcomes w/ $x$ heads}}{\# \text{outcomes in total}} = \frac{\displaystyle{{4 \choose x}}}{\displaystyle{2^4}}.
\end{equation*}
\end{frame}

%-------------------------------------------------------
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
So, the number of outcomes satisfying $X=2$ is

\begin{flalign*}
\frac{4!}{2!\; 2!}
&= \frac{4!}{2!\; (4-2)!}
= {4 \choose 2}
= \mbox{ ``4 choose 2"} & \\
& = 6 \\
& = \text{the number of ways to arrange 2 heads among 4 flips}
\end{flalign*}
\vspace{-0.5cm}
\begin{align*}
S=\big\{
&HHHH,\\
&HHHT,HHTH,HTHH,THHH,\\
&HHTT,HTHT,HTTH,THTH,TTHH,THHT,\\
&HTTT, THTT,TTHT,TTTH,\\
&TTTT
\big\}.
\end{align*}

\end{frame}
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
\vspace{0.5cm}

The number of ways to arrange $x$ heads among $4$ flips is
$\displaystyle{{4 \choose x}}$.
\vskip1.0cm

How many outcomes have 3 Heads?
\vskip0.35cm

\pause$\displaystyle{{4 \choose 3}=\frac{4!}{3!\;(4-3)!}=\frac{24}{6(1)}=4}$.
\end{frame}
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
\vspace{0.1cm}
Note that in real life, it's not quite true that the probability of having a boy $P(M)$ is equal to the probability of having a girl $P(F)$.

If $P(M)\ne P(F)$,
\vskip0.1cm
are all 4 outcomes with 3 females equally likely?
\vskip0.3cm


\quad ($FFFM, FFMF, FMFF, MFFF$)
\vskip1.2cm

What is $P(FMFF)$?
\end{frame}
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
\vspace{0.1cm}

What is $P(FMFF)$?
\begin{align*}
P(FMFF) &= P(MFF\;|\;F)\times P(F) \\
& \hid{2-}{ = P(MFF)\times P(F)\hskip3.4cm (*) }\\
& \hid{3-}{ = [P(FF\;|\;M)\times P(M)]\times P(F)} \\
& \hid{4-}{ = P(FF)\times P(M) \times P(F)\hskip2.3cm (*)}\\
& \hid{5-}{ = [P(F\;|\;F)\times P(F)]\times P(M)\times P(F)}\\
& \hid{6-}{ = P(F)\times P(F)\times P(M)\times P(F)\hskip1.2cm (*)}\\
& \hid{7-}{ = P(F)^3\times P(M) }
\end{align*}
\hid{2-}{$(*)$ by independence of gender by birth order}
\vskip0.25cm

\hid{8-}{Finally, $\displaystyle{P(X=3)={4 \choose 3}\;P(F)^3\; P(M)}$}

\end{frame}
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
\vspace{0.5cm}

So, even if genders are not equally likely,

we can find probabilities for $X = 0, 1, 2, 3,$ and 4.\\
\vskip1.0cm
First, let $p=P(F)$\hskip1cm ($0<p<1$)
\vskip0.4cm
then $P(M)=1-p$,
\vskip0.4cm
where $0 \le p \le 1$ is the probability of ``success"
(female birth).

\end{frame}
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
\begin{align*}
P(X=3)&={4 \choose 3}\;P(F)^3\; P(M)^{1} \\
 &={4 \choose 3}\;p^3\; (1-p)^{4-3}=6\;p^3\;(1-p)\\[+0.2cm]
P(X=0)&=\hid{2-}{{4 \choose 0}\;p^0\; (1-p)^{4-0}=(1-p)^4}\\[+0.2cm]
P(X=1)&=\hid{3-}{{4 \choose 1}\;p^1\; (1-p)^{4-1}=4\;p\;(1-p)^3}\\[+0.2cm]
P(X=2)&=\hid{4-}{{4 \choose 2}\;p^2\; (1-p)^{4-2}=4\;p^2\;(1-p)^2}\\[+0.2cm]
P(X=4)&=\hid{5-}{{4 \choose 4}\;p^4\; (1-p)^{4-4}=p^4}
\end{align*}

\end{frame}
%-------------------------------------------------------
\begin{frame}{Using counting rules to determine probabilities}
\vspace{0.1cm}

Does this agree with our earlier work when $P(F)=p(M)=0.5$?
\vskip0.3cm

Then, $p=0.5$ and $(1-p)=0.5$.
\begin{align*}
P(X=0)&= (1-p)^4 &= (0.5)^4 &= 1/16\\
P(X=1)&= 4\;p\;(1-p)^3 &= 4\;(0.5)\;(0.5)^3 &= 4/16\\
P(X=2)&= 4\;p^2\;(1-p)^2 &= 6\;(0.5)^2\;(0.5)^2 &= 6/16\\
P(X=3)&= 6\;p^3\;(1-p) &= 4\;(0.5)^3\;(0.5) &= 4/16\\
P(X=4)&= p^4 &= (0.5)^4 &= 1/16
\end{align*}
\vskip0.5cm
Same probability distribution as before!\hskip1cm
That's comforting.

\end{frame}
%-------------------------------------------------------
\begin{frame}{Bernoulli and binomial probability distributions}
A Bernoulli random variable models a very simple process.\\
For example, $Y=$ the number of females in one birth. Then,
\vskip0.2cm

$Y = \begin{cases}
     1 & \text{if child is female}\\
     0 & \text{if child is male}
  \end{cases}$
\vskip0.2cm

where $P(Y=1)=P(F)=p,$ and $P(Y=0)=P(M)=1-p.$
\vskip0.4cm

%The range of $Y$ is just $\{0,1\}.$
%\vskip0.3cm
We say that $Y\sim$ Bernoulli($p$), \\
or $Y$ is a Bernoulli random variable with ``success" probability $p$.

\end{frame}
%-------------------------------------------------------
\begin{frame}{Bernoulli and binomial probability distributions}
Let $Y=$ \# of ``successes" in one Bernoulli ($p$) ``trial"

Then $Y\sim$ Bernoulli($p$) and the pmf for $Y$ is
$$f(y) = \hid{2-}{p^y\;(1-p)^{1-y}\qquad \mbox{for } y=0,1}$$
\vskip0.5cm

Let $X=$ \# of ``successes" in $n$ independent Bernoulli ($p$) ``trials"
\vskip0.2cm

Then, we say that
$X\sim$ binom($n,p$), \\
or $X$ is a binomial random variable with $n$ \textbf{independent} trials and success probability $p$
and the pmf for $X$ is
$$f(x) = \hid{3-}{{n\choose x}\; p^x\;(1-p)^{n-x}\qquad \mbox{for } x=0,1,...,n}$$

\end{frame}
%----------------------------------
  \begin{frame}{The Binomial Expansion}
The coefficients in the expansion match those in Pascal's Triangle:
\begin{align*}
(w+y)^1&=1w^1+~1y^1\\
(w+y)^2&=1w^2+2wy+~~1y^2\\
(w+y)^3&=1w^3+3w^2y+~3w^1y^2+~1y^3\\
(w+y)^4&=1w^4+4w^3y+~6w^2y^2+4wy^3+1y^4\\
(w+y)^5&=1w^5+5w^4y+10w^3y^2+10w^2y^3+5wy^4+1y^5
\end{align*}
\tabcolsep=0pt
\begin{tabular}{ccccccccccc}
&&&&&${0\choose 0}$ &   &   &   &   &\\[-2pt]
 &&&&${1\choose 0}$ && ${1\choose 1}$ &   &   &   &\\[-2pt]
  &&&${2\choose 0}$ && ${2\choose 1}$ && ${2\choose 2}$ &&&\\[-2pt]
   &&${3\choose 0}$ && ${3\choose 1}$ && ${3\choose 2}$ && ${3\choose 3}$ &&\\[-2pt]
    &${4\choose 0}$ && ${4\choose 1}$ && ${4\choose 2}$ && ${4\choose 3}$ && ${4\choose 0}$&\\[-2pt]
     ${5\choose 0}$ && ${5\choose 1}$ && ${5\choose 2}$ && ${5\choose 3}$ && ${5\choose 4}$&&${5\choose 5}$\\[-2pt]
&&&&&$\vdots$
\end{tabular}
$=$
\tabcolsep=2pt
\begin{tabular}{ccccccccccc}
  &   &   &   &   & 1 &   &   &   &   &\\[-2pt]
  &   &   &   & 1 &   & 1 &   &   &   &\\[-2pt]
  &   &   & 1 &   & 2 &   & 1 &   &   &\\[-2pt]
  &   & 1 &   & 3 &   & 3 &   & 1 &   &\\[-2pt]
  & 1 &   & 4 &   & 6 &   & 4 &   & 1 &\\[-2pt]
1 &   & 5 &   & 10&   & 10&   & 5 &   & 1\\[-2pt]
&&&&&$\vdots$
\end{tabular}
\end{frame}
%----------------------------------

\begin{frame}{The Binomial Expansion}

In general,\\
$\displaystyle{
(w+y)^n =
\underbrace{(w+y)(w+y)\ldots(w+y)}_{n\mbox{\small\ factors}}=\sum_{x=0}^n {n\choose x} w^x y^{n-x}
}$
\vskip0.3cm

General idea:\\
$\displaystyle{
w^5 y^3 =  w w w w w y y y = w w w w y w y y = \cdots = y y y w w w w w
}$
\vskip0.3cm
\pause

This result guarantees that the binomial RV has a valid pmf.
\vskip0.3cm
To see this, let $w=p, y=(1-p).$  Then,
$\displaystyle{\sum_{x=0}^n {n\choose x} p^x (1-p)^{n-x}}$
$\displaystyle{
=\sum_{x=0}^n {n\choose x} w^x y^{n-x}
= (w+y)^n = (p + (1-p))^n = 1^n = 1
}$
\vskip0.3cm

The probabilities for any valid pmf must sum to 1.

\end{frame}
%-------------------------------------------------------
\begin{frame}{Multiple Random Variables, Same Sample Space}

We can define several random variables on this same experiment (the same
sample space):
\begin{align*}
X &= \text{Number of female children}\\
Y &= \text{Number of male children before the first female child is born}\\
Z &= \begin{cases}
     1 & \text{if more female children than male}\\
     0 & \text{otherwise}
     \end{cases}
\end{align*}

{\scriptsize
\begin{center}
\begin{tabular}{c|ccc|c|ccc|c|ccc}
    Outcome & $X$ & $Y$ & $Z$
  & Outcome & $X$ & $Y$ & $Z$
  & Outcome & $X$ & $Y$ & $Z$ \\
\hline
FFFF & 4 & 0 & 1 & FFMM & 2 & 0 & 0 & FMMM & 1 & 0 & 0 \\
     &   &   &   & FMFM & 2 & 0 & 0 & MFMM & 1 & 1 & 0 \\
FFFM & 3 & 0 & 1 & FMMF & 2 & 0 & 0 & MMFM & 1 & 2 & 0 \\
FFMF & 3 & 0 & 1 & MFMF & 2 & 1 & 0 & MMMF & 1 & 3 & 0 \\
FMFF & 3 & 0 & 1 & MFFM & 2 & 1 & 0 &      &   &   &   \\
MFFF & 3 & 1 & 1 & MMFF & 2 & 2 & 0 & MMMM & 0 & 4 & 0 \\
\end{tabular}
\end{center}
}

\end{frame}
%-------------------------------------------------------
\begin{frame}{Multiple pmfs, Same Sample Space}

We can define several random variables and their corresponding
pmfs on this same experiment (the same sample space):
{\scriptsize
\begin{center}
\begin{tabular}{c|ccc|c|ccc|c|ccc}
    Outcome & $X$ & $Y$ & $Z$
  & Outcome & $X$ & $Y$ & $Z$
  & Outcome & $X$ & $Y$ & $Z$ \\
\hline
FFFF & 4 & 0 & 1 & FFMM & 2 & 0 & 0 & FMMM & 1 & 0 & 0 \\
     &   &   &   & FMFM & 2 & 0 & 0 & MFMM & 1 & 1 & 0 \\
FFFM & 3 & 0 & 1 & FMMF & 2 & 0 & 0 & MMFM & 1 & 2 & 0 \\
FFMF & 3 & 0 & 1 & MFMF & 2 & 1 & 0 & MMMF & 1 & 3 & 0 \\
FMFF & 3 & 0 & 1 & MFFM & 2 & 1 & 0 &      &   &   &   \\
MFFF & 3 & 1 & 1 & MMFF & 2 & 2 & 0 & MMMM & 0 & 4 & 0 \\
\end{tabular}
\end{center}
}

{\small
\begin{tabular}{r|ccccc}
$x$ & 0&1&2&3&4\\
\hline
$f(x)$ & 1/16&4/16&6/16&4/16&1/16\\
\end{tabular}
\vskip0.25cm

\begin{tabular}{r|ccccc}
$y$ &
\hid{2-}{0}&\hid{3-}{1}&\hid{4-}{2}&\hid{4-}{3}&\hid{4-}{4}\\
\hline
$f(y)$ &
\hid{2-}{8/16}&\hid{3-}{4/16}&\hid{4-}{2/16}&\hid{4-}{1/16}&\hid{4-}{1/16}\\
\end{tabular}
\vskip0.25cm

\begin{tabular}{r|cc}
$z$ & \hid{5-}{0}&\hid{6-}{1}\\
\hline
$f(z)$ & \hid{5-}{11/16}&\hid{6-}{5/16}\\
\end{tabular}
}
\end{frame}
%-------------------------------------------------------
\begin{frame}{Probability Histograms}

\begin{tabular}{r|ccccc}
$x$ & 0&1&2&3&4\\
\hline
$f(x)$ & 1/16&4/16&6/16&4/16&1/16\\
\end{tabular}
\vskip0.2cm

\begin{tabular}{r|ccccc}
$y$ & 0&1&2&3&4\\
\hline
$f(y)$ & 8/16&4/16&2/16&1/16&1/16\\
\end{tabular}
\vskip0.2cm

\begin{tabular}{r|cc}
$z$ & 0&1\\
\hline
$f(z)$ & 11/16&5/16\\
\end{tabular}
\vskip-0.2cm

<<echo=FALSE, fig.height=3.0>>=
par(mfrow=c(1,3), oma=c(0,0,0,0)+0.01, mar=c(4,4,1,1)+0.01, cex=1.20)
x <- 0:4;  fx <- c(1,4,6,4,1)/16
names(fx) <- as.character(x)
barplot(fx, space=0, col="grey", xlab="x", ylab="f(x)=probability", ylim=c(0,11/16))
y <- 0:4;   fy <- c(8,4,2,1,1) / 16
names(fy) <- as.character(y)
barplot(fy, space=0, col="grey", xlab="y", ylab="f(y)=probability", ylim=c(0,11/16))
z <- 0:4;   fz <- c(11,5,0,0,0) / 16
names(fz) <- as.character(z)
barplot(fz, space=0, col="grey", xlab="z", ylab="f(z)=probability", ylim=c(0,11/16))
@

%\includegraphics[width=10cm]{ProbHistograms}

\end{frame}
%-------------------------------------------------
% \begin{frame}{Expected Value (Mean)}
%
% What is the average value for each random variable?
% \begin{align*}
% \mbox{Mean of\ } X &= \mu = ``\mbox{myoo}"\\
% &= \mbox{Expected value of\ } X = E(X)\\
% &= \mbox{Balancing point of the probability distribution of\ } X
% \end{align*}
% What is the expected value of $X?$
% %\begin{tabular}{r|ccccc}
% %$x$ & 0&1&2&3&4\\
% %\hline
% %$f(x)$ & 1/16&4/16&6/16&4/16&1/16\\
% %\end{tabular}
%
% <<echo=FALSE, fig.height=2.0>>=
% par(mfrow=c(1,1), oma=c(0,0,0,0)+0.01, mar=c(4,4,1,1)+0.01, cex=1.20)
% x <- 0:4;  fx <- c(1,4,6,4,1)/16
% names(fx) <- as.character(x)
% barplot(fx, space=0, col="grey", xlab="x", ylab="f(x)=probability", ylim=c(0,11/16))
% @
% \vskip-0.3cm
% $E(X) = \hid{2-}{2 \;\;\mbox{\ (by symmetry)}}$\qquad
% \hid{3-}{But $Y$ and $Z$ distns not symmetric.}
% \end{frame}
% %%%%%%%%%%%%%%%%
% \begin{frame}{Expected Value (Mean of a r.v.)}
%
%
% {\bf Developing a general formula for expected value:}\\
% Suppose that 16, 4-child families are chosen at random.\\
% \medskip
% We would ``expect" to observe about\\
% 1 family with all girls $(FFFF)$,\\
% 4 families with 3 girls $(FFFM, FFMF, FMFF, MFFF)$,\\
% 6 families with 2 girls,\\
% 4 families with 1 girl, and\\
% 1 family with 0 girls.
% \begin{align*}
% & \mbox{Average (in the ``long run" using counting theory)}\\
% & =\frac{\mbox{total \# of girls}}{\mbox{total \# of families}}\\
% &= \frac{4+3+3+3+3+2+2+2+2+2+2+1+1+1+1+0}{16}\\
% &= 32/16 = 2
% \end{align*}
%
% \end{frame}
% %%%%%%%%%%%%%%%%
% \begin{frame}{Expected Value (Mean of a r.v.)}
%
% Then, average number of girls per family (mean of $X$)\\
% can be rewritten...
% \begin{align*}
% & \mbox{Average}\\
% &= \frac{4 (1 \mbox{\ family}) + 3(4 \mbox{\ families}) + 2(6 \mbox{\ fam})
%          + 1(4 \mbox{\ fam}) + 0(1 \mbox{\ fam})}{16}\\
% &= 4\times\frac{1}{16} + 3\times\frac{4}{16} + 2\times\frac{6}{16}
%    + 1\times\frac{4}{16} + 0\times\frac{1}{16}\\
% &= \sum_{x=0}^4 x\; P(X=x)\\
% &= \sum_{x=0}^4 x\; f(x)
% \end{align*}
%
% \end{frame}
% %%%%%%%%%%%%%%%%
% \begin{frame}{Expected Value (Mean of a r.v.)}
% \vspace{0.1cm}
%
% {\bf Definition of Expected Value:}
% \begin{align*}
% E(X) &= \text{weighted average of all possible outcomes $x$}\\\\
% E(X) &= \sum_{\text{all\ } x}\; x\; P(X=x)\\[+0.3cm]
% &= \sum_x\; x\; f(x)\\[+0.3cm]
% &= \mu
% \end{align*}
%
% This is a long-run average, the average of a random variable,\\
% and the average for the probability model of the population (or process)
% distribution from which the data come.
%
% \end{frame}
%%%%%%%%%%%%%%%%

\begin{frame}{Recall: Mean}
\begin{block}{Mean (Expected Value)}
Suppose $X$ is a discrete random variable, then
\begin{align*}
  \text{\alert{mean} of } X &= E[X] \\
  &= \sum_{\text{all } x}xP(X = x)\\
  &=\sum_{\text{all } x}xf(x)\\
  &=\mu
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Expected Value (Mean of a r.v.)}

Find  expected value (mean) of random variable $Y$.$


$Y =$ \# of male children before the first female child is born

\begin{tabular}{r|ccccc}
$y$ &
0&1&2&3&4\\
\hline
$f(y)$ &
8/16&4/16&2/16&1/16&1/16\\
\end{tabular}
<<echo=FALSE, fig.height=3.0>>=
par(mfrow=c(1,1), oma=c(0,0,0,0)+0.01, mar=c(4,4,1,1)+0.01, cex=1.20)
y <- 0:4;   fy <- c(8,4,2,1,1) / 16
names(fy) <- as.character(y)
barplot(fy, space=0, col="grey", xlab="y", ylab="f(y)=probability", ylim=c(0,11/16))
@
\vskip-1.45cm
\begin{align*}
E(Y) &=
\hid{2-}{\sum_{y=0}^4 y f(y)}\\
&= \hid{2-}{(0)8/16 + (1)4/16 + (2)2/16 + (3)1/16 + (4)1/1 = 15/16}
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expected Value (Mean of a r.v.)}

The random variable $Z$ is a Bernoulli r.v.
%\quad{\tiny Top Hat: $E(Z)$}
$Z = \begin{cases}
    1 & \text{if more female children than male}\\
    0 & \text{otherwise}
    \end{cases}$

\begin{tabular}{r|cc}
$z$ & 0&1\\
\hline
$f(z)$ & 11/16&5/16\\
\end{tabular}
<<echo=FALSE, fig.height=2>>=
par(mfrow=c(1,1), oma=c(0,0,0,0)+0.01, mar=c(4,4,1,1)+0.01, cex=1.20)
z <- 0:4;   fz <- c(11,5,0,0,0) / 16
names(fz) <- as.character(z)
barplot(fz, space=0, col="grey", xlab="z", ylab="f(z)=probability", ylim=c(0,11/16))
@
\vskip-0.5cm
Make a guess.  Approximate the average outcome for $Z$.\\
$E(Z) = $
\hid{2-}{$\displaystyle{\sum_{z=0}^1 z f(z) = (0)11/16 + (1)5/16 = 5/16.}$}

\end{frame}
%%%%%%%%%%%%%%%%
\begin{frame}{Expected Value of a Bernoulli Random Variable}
\vspace{0.2cm}

The random variable $Z$ is a Bernoulli r.v.
\vskip0.2cm
$Z = \begin{cases}
    1 & \text{if more female children than male = ``success"}\\
    0 & \text{otherwise = ``failure"}
    \end{cases}$

Probability mass function (PMF):\hskip1.5cm
\begin{tabular}{r|cc}
$z$ & 0&1\\
\hline
$f(z)$ & $(1-p)$ & $p$\\
\end{tabular}
where $p=5/16$ = probability of a success.

\begin{align*}
\mu_Z = E(Z) &= \text{weighted average of all possible outcomes $x$}\\[+0.3cm]
E(Z) &= \sum_{\text{all\ } z}\; z\; P(Z=z)
= \sum_{z=0,1}\; z\; P(Z=z)\\[+0.1cm]
&= (0)(1-p) + (1)(p) = p = 5/16.
\end{align*}

\end{frame}
%%%%%%%%%%%%%%%%
\begin{frame}{Expected Value of a Bernoulli Random Variable}
\vspace{0.2cm}

The random variable $Z$ is a Bernoulli r.v.
\vskip0.2cm
$Z = \begin{cases}
    1 & \text{if more female children than male = ``success"}\\
    0 & \text{otherwise = ``failure"}
    \end{cases}$


Prob mass function (PMF):\hfill
$f(z) = p^z\;(1-p)^{1-z}\quad \mbox{for } z=0,1$

where $p=5/16$ = probability of a success.

\begin{align*}
\mu_Z = E(Z) &= \text{weighted average of all possible outcomes $x$}\\[+0.3cm]
E(Z) &= \sum_{\text{all\ } z}\; z\; f_Z(z)
= \sum_{z=0}^1\; z\; p^z\;(1-p)^{1-z}\\[+0.1cm]
&= (0)p^0(1-p)^{1-0} + (1)p^1(1-p)^{1-1} = p = 5/16.
\end{align*}

\end{frame}
%%%%%%%%%%%%%%%%
\begin{frame}{The Binomial Setting}
\vspace{0.2cm}

\begin{enumerate}[1.]
\item
There is a fixed number of observations $n$.
\item
The $n$ observations are all independent.
\item
Each observation falls into one of just two categories.

For convenience, called ``success" and ``failure"
\item
The probability of a success ($p$) \\
is the same for each observation.
\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%
\begin{frame}{The Binomial Distribution}
\vspace{0.2cm}

Let $X$ = the count of successes in a Binomial setting
\vskip0.5cm

Then, the following statements are equivalent:
\begin{itemize}
\item
$X$ has a Binomial distribution with parameters $n$ and $p$.
\item
$X$ is a Binomial($n,p$) random variable.
\item
$X\sim$ Binomial($n,p$).
\item
$X$ is the sum of $n$ independent Bernoulli r.v. (***)
\item
The probability mass function (pmf) for random variable $X$ is
$$f(x) = {n\choose x}\; p^x\;(1-p)^{n-x}\qquad \mbox{for } x=0,1,...,n$$
\end{itemize}

$n=$ number of observations (sample size)

$p=$ probability of success for any one observation


\end{frame}
%%%%%%%%%%%%%%%%
\begin{frame}{Expected Value of a Binomial Random Variable}
\vspace{0.2cm}

Is $X$ a Binomial($n,p$) random variable?
\vskip0.5cm

Without studying, you plan to
randomly guess each quiz question.
\begin{enumerate}[(1)]
\item
$X = $ number of correct answers in a quiz with 10 questions
and 5 choices per question (A, B, C, D, E).
\item
$X = $ number of correct answers in a quiz with 100 questions
and 4 choices per question (A, B, C, D).
\item
$X = $ number of correct answers in a quiz with 50 questions
and 4 choices per question (A, B, C, D).
\end{enumerate}
In each case, how many correct answers do you expect to get?

\vskip0.5cm
Let $X=$ \# of ``successes" in $n$ independent Bernoulli ($p$) ``trials"

Is $X$ Binomial($n,p$)?\hskip1cm
What is $E(X)$?


\end{frame}
%%%%%%%%%%%%%%%%
\begin{frame}{Expected Value of a Binomial Random Variable}
\vspace{0.2cm}

If $X$ is a Binomial($n,p$) random variable, $E(X)=np$.

\begin{align*}
E(X) &= \sum_{\text{all } x}\; x\; f(x)\\
&= \sum_{x=0}^n\;\; x\;\;{n\choose x}\; p^x\;(1-p)^{n-x}\\
&= np
\end{align*}

\end{frame}


\end{document}
