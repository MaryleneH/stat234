\documentclass{beamer}
\usepackage{amsmath,algorithm,algorithmic,graphicx,amsfonts,amsthm,color,pgf,tikz,wrapfig,amsfonts,multicol,wasysym,animate, appendixnumberbeamer}
\beamertemplatenavigationsymbolsempty
\useoutertheme[subsection=false]{miniframes}
\usetheme[progressbar=frametitle]{metropolis}
\metroset{block=fill}

\providecommand{\code}[1]{}
\renewcommand{\code}[1]{{\color{blue!80!black}\texttt{#1}}}
\def\hid#1#2{\onslide<#1>{#2}}


\title{Multiple Random Variables}
\author{David Gerard}
\date[\Sexpr{Sys.Date()}]{\Sexpr{Sys.Date()}}


\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
options(format.R.blank=FALSE)
options(width=60)
options(continue=" ")
options(replace.assign=TRUE)
options(scipen=8, digits=4)
opts_chunk$set(
  dev="pdf",
  fig.align='center',
  fig.width=7,
  fig.height=4,
  fig.pos='H',
  fig.show='asis',
  out.width='0.99\\linewidth',
  par=TRUE,
  tidy=FALSE,
  tidy.opts=list(width.cutoff=50),
  prompt=FALSE,
  comment=NA
)
@

\begin{frame}[fragile]
  \titlepage
\end{frame}

\begin{frame}{Learning Objectives}
\begin{itemize}
\item Joint/conditional/marginal distributions of random variables.
\item Independence of random variables.
\item Covariance of random variables.
\item Mean and variance of sum of random variables.
\end{itemize}
\end{frame}

\begin{frame}{Motivating Example}
\begin{itemize}
\item Consider experiment of tossing two fair dice.
\item $\text{Sample space} = S = \{(1,1),\ (1,2),\ (1,3),\ldots,\ (5,6),\ (6,6)\}$.
\item $P(z) = 1/36$ for all $z \in S$.
\item Let $X = $ sum of the two dice.
\item Let $Y = |\text{difference of the two dice}|$.
\item For each of the 36 outcomes in the sample space, we have corresponding values of $X$ and $Y$.
\item We can write out the joint distribution probability mass function of $X$ and $Y$.
\end{itemize}
\end{frame}

\begin{frame}{joint pmf}
\begin{block}{joint pmf}
Let $X$ and $Y$ be two discrete bivariate random variables. Then the function $f(x, y): \mathbb{R}^2 \rightarrow \mathbb{R}$ defined by $P(X = x \text{ and } Y = y)$ is called the \alert{joint probability mass function} of $X$ and $Y$.
\end{block}
\end{frame}

\begin{frame}{Dice Example}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
    &   & 2              & 3              & 4              & 5              & 6              & 7              & 8              & 9              & 10             & 11             & 12            \\
    & 0 & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$\\
    & 1 &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &               \\
$y$ & 2 &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &               \\
    & 3 &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &               \\
    & 4 &                &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &                &               \\
    & 5 &                &                &                &                &                & $\frac{1}{18}$ &                &                &                &                &               \\
\end{tabular}

$f(x, y) = 0$ for all other $(x, y)$.

E.g.
\begin{align*}
f(3, 1) &= P(X = 3, Y = 1) = P(\{(2, 1), (1, 2)\})\\
&= P((2, 1)) + P((1, 2)) \text{ (disjoint events)}\\
&= \frac{1}{36} + \frac{1}{36} = \frac{1}{18}.
\end{align*}
\end{frame}

\begin{frame}{Property of joint pmf}

Some properties readily generalize to the multiple variable case:
\begin{itemize}
\item $\sum_{x}\sum_{y}f(x, y) = 1$
\item $0 \leq f(x, y) \leq 1$ for all $(x, y)$.
\end{itemize}
\end{frame}

\begin{frame}{Marginal Distributions}
From the joint pmf, we can find the pmf of \textbf{just} $X$ and the pmf of \textbf{just} $Y$.

To find the pmf of $X$ at a particular $x$, we find every pair $(x, y)$ and add up their joint probabilities.
\begin{align*}
f_X(x) = \sum_y f(x, y) = \sum_y P(X = x \text{ and } Y = y).
\end{align*}
Similarly
\begin{align*}
f_Y(y) = \sum_x f(x, y) = \sum_x P(X = x \text{ and } Y = y).
\end{align*}

Even though $f_X$ and $f_Y$ are just the pmf's of $X$ and $Y$, we call them \alert{marginal} pmfs when in the context of multiple variables.

\end{frame}

\begin{frame}{Dice Example}
\begin{table}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
    &   & 2              & 3              & 4              & 5              & 6              & 7              & 8              & 9              & 10             & 11             & 12            \\
    & 0 & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$\\
    & 1 &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &               \\
$y$ & 2 &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &               \\
    & 3 &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &               \\
    & 4 &                &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &                &               \\
    & 5 &                &                &                &                &                & $\frac{1}{18}$ &                &                &                &                &               \\
\end{tabular}
\begin{tikzpicture}[overlay]
 \draw[blue, line width = 0.5mm] (0.35, 2) ellipse (0.3cm and 2cm);
\end{tikzpicture}
\end{table}
\begin{align*}
P(X = 6) &= f_x(6) = f(6, 0) + f(6, 3) + f(6, 4) \\
&= 1/36 + 1/18 + 1/18 = 5 / 36
\end{align*}
\end{frame}

\begin{frame}{Dice Example}
\begin{table}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
    &   & 2              & 3              & 4              & 5              & 6              & 7              & 8              & 9              & 10             & 11             & 12            \\
    & 0 & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$\\
    & 1 &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &               \\
$y$ & 2 &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &               \\
    & 3 &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &               \\
    & 4 &                &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &                &               \\
    & 5 &                &                &                &                &                & $\frac{1}{18}$ &                &                &                &                &               \\
\end{tabular}
\begin{tikzpicture}[overlay]
 \draw[blue, line width = 0.5mm] (0.4, 1.93) ellipse (6cm and .33cm);
\end{tikzpicture}
\end{table}
\begin{align*}
P(Y = 2) &= f_y(2) = f(4, 2) + f(6, 2) + f(8, 2) + f(10, 2) \\
&= 1/18 + 1/18 + 1/18 + 1/18 = 4 / 36
\end{align*}
\end{frame}

\begin{frame}{Conditional Distribution}
\begin{block}{Conditional pmf}
Let $X$ and $Y$ be discrete random variables. For any $x$ such that $P(X = x) = f_X(x) > 0$, the \alert{conditional pmf} of $Y$ given $X = x$ is
\begin{align*}
f_{Y|X}(y|x) = \frac{P(X = x \text{ and } Y = y)}{P(X = x)} = \frac{f(x, y)}{f_X(x)}.
\end{align*}
Similarly
\begin{align*}
f_{X|Y}(x|y) = \frac{P(X = x \text{ and } Y = y)}{P(Y = y)} = \frac{f(x, y)}{f_Y(y)}.
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Dice Example i: $f(x|y = 2)$}
\begin{table}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
    &   & 2              & 3              & 4              & 5              & 6              & 7              & 8              & 9              & 10             & 11             & 12            \\
    & 0 & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$ &                & $\frac{1}{36}$\\
    & 1 &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &               \\
$y$ & 2 &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &               \\
    & 3 &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &               \\
    & 4 &                &                &                &                & $\frac{1}{18}$ &                & $\frac{1}{18}$ &                &                &                &               \\
    & 5 &                &                &                &                &                & $\frac{1}{18}$ &                &                &                &                &               \\
\end{tabular}
\begin{tikzpicture}[overlay]
 \draw[blue, line width = 0.5mm] (0.4, 1.93) ellipse (6cm and .33cm);
\end{tikzpicture}
\end{table}
\end{frame}

\begin{frame}{Dice Example ii: $f(x|y = 2)$}
\begin{table}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
&&2&3&4&5&6&7&8&9&10&11&12\\
&0&&&&&&&&&&&\\
&1&&&&&&&&&&&\\
$y$&2&&&$\frac{1}{18}$&&$\frac{1}{18}$&&$\frac{1}{18}$&&$\frac{1}{18}$&&\\
&3&&&&&&&&&&&\\
&4&&&&&&&&&&&\\
&5&&&&&&&&&&&\\
\end{tabular}
\begin{tikzpicture}[overlay]
 \draw[blue, line width = 0.5mm] (-5, -0.25) ellipse (6cm and .33cm);
\end{tikzpicture}
\end{table}

Intuition: Normalize selected row to sum to 1 so that they are probabilities.
\end{frame}

\begin{frame}{Dice Example ii: $f(x|y = 2)$}
\begin{table}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
&&2&3&4&5&6&7&8&9&10&11&12\\
&0&&&&&&&&&&&\\
&1&&&&&&&&&&&\\
$y$&2&&&$\frac{1}{4}$&&$\frac{1}{4}$&&$\frac{1}{4}$&&$\frac{1}{4}$&&\\
&3&&&&&&&&&&&\\
&4&&&&&&&&&&&\\
&5&&&&&&&&&&&\\
\end{tabular}
\begin{tikzpicture}[overlay]
 \draw[blue, line width = 0.5mm] (-5, -0.25) ellipse (6cm and .33cm);
\end{tikzpicture}
\end{table}

\begin{align*}
\frac{1}{18} + \frac{1}{18} + \frac{1}{18} + \frac{1}{18} &= 4/18\\
\frac{1/18}{4/18} &= \frac{1}{4}
\end{align*}
\end{frame}

\begin{frame}
\begin{table}
\begin{tabular}{c|cccccccccccc}
 $f(x, y)$&&             &                &                &                &                & $x$            &                &                &                &                &               \\
\hline
&&2&3&4&5&6&7&8&9&10&11&12\\
&0&&&&&&&&&&&\\
&1&&&&&&&&&&&\\
$y$&2&&&$\frac{1}{4}$&&$\frac{1}{4}$&&$\frac{1}{4}$&&$\frac{1}{4}$&&\\
&3&&&&&&&&&&&\\
&4&&&&&&&&&&&\\
&5&&&&&&&&&&&\\
\end{tabular}
\begin{tikzpicture}[overlay]
 \draw[blue, line width = 0.5mm] (-5, -0.25) ellipse (6cm and .33cm);
\end{tikzpicture}
\end{table}
So
\begin{align*}
f_{X|Y}(x|y = 2)
=
\begin{cases}
\frac{1}{4} & \text{ if } x \in \{4, 6, 8, 10\}\\
0 & \text{ otherwise}.
\end{cases}
\end{align*}
\end{frame}

\begin{frame}{Independence}
\begin{block}{independence}
Two random variables $X$ and $Y$ are \alert{independent} if for every $x\in\mathbb{R}$ and $y\in\mathbb{R}$
\begin{align*}
f(x, y) = f_X(x)f_Y(y).
\end{align*}
\end{block}
\begin{itemize}
\item This is equivalent to $f_{X|Y}(x|y) = f_X(x)$ and $f_{Y|X}(y|x) = f_Y(y)$ for all $x$ and $y$.
\item I.e. knowing $y$ doesn't change the distribution of $x$ and knowing $x$ doesn't change the distribution of $y$.
\item So knowing $y$ doesn't tell you anything new about $x$ and knowing $x$ doesn't tell you anything new about $y$.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Extension of law of unconscious statistician}

With two random variables $X,Y$ we can create a new random variable $Z=g(X,Y)$
with $g: R \times R \rightarrow R$.

For example $g(x,y) = x+y$ or $g(x,y)=x\cdot y$.

\bigskip
Again we could try to compute the distribution of $Z$ - hard  (conscious statistician).

\bigskip
Or we have:
\begin{block}{Law of Unconscious Statistician}
\begin{align*}
E (g(X,Y)) = \sum_{x \in R_x, y \in R_y} g(x,y) f_{X,Y} (x,y)
\end{align*}
\end{block}
\end{frame}

\begin{frame}{Consequences of law of unconscious statistician i}

$E(X+Y) = E X + E Y.$


\begin{align*}
E(X+ Y) = & \sum_{x \in R_x, y \in R_y} (x+y) f_{X,Y}(x,y) \\
= & \sum_{x \in R_x, y \in R_y} x f_{X,Y}(x,y) + \sum_{x \in R_x, y \in R_y}  y f_{X,Y}(x,y) \\
\mbox{Why? \ } = & \sum_{x \in R_x} x f_{X}(x) + \sum_{y \in R_y} y f_Y(y) \\
= & EX + EY.
\end{align*}

More generally, if $X_1,\ldots,X_N$ are random variables:

$E \left(\sum_{n=1}^N X_n \right) = \sum_{n=1}^N E X_n .$

To know the mean of the sum we don't need to know the joint distribution!
\end{frame}

\begin{frame}{Consequences of law of unconscious statistician ii}
\bigskip

If $X,Y$ are independent: $E (X \cdot Y) = E(X) \cdot E(Y)$

\begin{align*}
E(X \cdot Y) = & \sum_{x \in R_x, y \in R_y} x \cdot y \cdot f_{X,Y}(x,y) \\
\mbox{Independence: \ } = & \sum_{x \in R_x, y \in R_y} x \cdot y \cdot f_X(x) f_Y(y) \\
\mbox{Why? \ } = & \sum_{x \in R_x} x f_X(x) \sum_{y \in R_y}y f_Y(y) \\
= & EX \cdot EY.
\end{align*}
\end{frame}

\begin{frame}{Continuous Random Variables}
This entire discussion on multiple random variables holds for continuous random varaibles by replacing sums with integrals.
\begin{itemize}
\item $\int_x\int_y f(x, y)dx dy = 1$
\item $f_X(x) = \int_y f(x, y) dy$ is the marginal density of $X$
\item $f_Y(y) = \int_x f(x, y) dx$ is the marginal density of $Y$
\item $f_{Y|X}(y|X = x) = f(x, y) / f_X(x)$ is the conditional density of $Y$ given $X = x$
\item $f_{X|Y}(x|Y = y) = f(x, y) / f_Y(y)$ is the conditional density of $X$ given $Y = y$
\item $E[g(X, Y)] = \int_x\int_y g(x, y) f(x, y) dx dy$
\item $E[X + Y] = E[X] + E[Y]$
\item $E[XY] = E[X]E[Y]$ \alert{ if $X$ and $Y$ are independent}
\end{itemize}


\end{frame}

% \begin{frame}
% \parbox[t]{0pt}{}
%
% \bigskip
%
% Variance of a random variable $X$ with mean $\mu_X$  take $g(x) = (x-\mu_X)^2$.
%
% $Var(X) = E g(X).$
%
% \bigskip
% Show that:
%
% \medskip
% $Var(X) = EX^2 - \mu_X^2$
%
% $E(aX+b) = aE(X)+b.$
%
% $Var(aX+b) = a^2 Var(X).$
%
%
% \end{frame}
%
% \begin{frame}[allowframebreaks, fragile]{Covariance of two random variables}
%
% \bigskip
% {\bf Covariance} of two random variables $X,Y$ with means $\mu_X, \mu_Y$.
%
%
%
% Take $g(x,y) = (x-\mu_X) \cdot (y-\mu_Y).$
%
% $\Cov(X,Y) = E g(X,Y)$ - A measure of how the variables 'covary'.
%
% \bigskip
% $Cov(X,Y)>0 \  \longrightarrow$ when $X$ increases $Y$ tends to increase.
%
% $Cov(X,Y)<0 \ \longrightarrow$ when $X$ increases $Y$ tends to decrease.
%
% \bigskip
% Show that $\Cov(aX,bY) = a\cdot b Cov(X,Y).$
%
% Changing the units of a measurement will change covariance.
%
% \bigskip
% Correlation $\rho(X,Y)$ does not depend on units of measurement:
%
% $\rho(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$
%
% Show that: $\rho(aX,bY) = \rho(X,Y).$
%
% \newpage
%
% \parbox[t]{0pt}{}
%
% \bigskip
%
% {\bf Independence:}
%
%
% \bigskip
% Show that $\Cov(X,Y) = EXY - \mu_X \mu_Y.$
%
% Conclude: If $X,Y$ are independent $\Cov(X,Y)=0$. (Converse is not true.)
%
%
% \end{frame}
%
%
% \begin{frame}[allowframebreaks, fragile]{Variance of a sum of random variables}
%
%
% \begin{align*}
% Var(X+Y) =  & E[(X+Y)^2] - [E(X+Y)]^2 \\
%  =  & E[X^2 +2XY + Y^2] - [E(X)]^2-2E(X)E(Y) - [E(Y)]^2 \\
% =  & E(X^2) - [E(X)]^2 + E(Y^2) - [E(Y)]^2  \\
% & + 2E(XY) - 2 E(X) E(Y) \\
% =  &Var(X)+Var(Y) + 2 Cov(X,Y)
% \end{align*}
%
% \medskip
% Conclude: If $X,Y$ independent: $Var(X+Y) = Var(X) + Var(Y).$
%
% \medskip
% More generally, if $X_1,\ldots,X_N$ are independent then
%
% \medskip
% $Var(\sum_{n=1}^N X_n) = \sum_{n=1}^N Var(X_n)$.
% \end{frame}
%
% \begin{frame}[allowframebreaks, fragile]{Properties of sample average}
%
% We draw with replacement form a box $N$ times and record the number on
% each draw as $X_1,X_2, \ldots, X_N$. Because we draw with replacement we
% can {\it assume} that the variables $X_n$ are independent.
%
% Let $\mu_B$ be the {\it average of the box} and  let $\si^2_B$ be the mean square deviation
% (MSD) of the box.
%
% \medskip
%
% Recall that $E X_n = \mu_B$ and $Var X_n = \si^2_B$ for each $n$.
%
% Denote the  sample average as$ \overline{X} = \frac{1}{N} \sum_{n=1}^N X_n$.
%
% \begin{align*}
% E \overline{X} =  & \frac{1}{N} E \sum_{n=1}^N X_n \\
% \mbox{Why?}  \ = &\frac{1}{N} \sum_{n=1}^N E X_n \\
% \mbox{Why?}  \ = &\mu_B.
% \end{align*}
%
%
%
%
% \bigskip
%
% The variance of the sample average
%
% \begin{align*}
% Var( \overline{X}) = & Var( \frac{1}{N} \sum_{n=1}^N X_n) \\
% = & \frac{1}{N^2} Var (\sum_{n=1}^N X_n) \\
% \mbox{Independence:} \ = & \frac{1}{N^2} \sum_{n=1}^N Var (X_n) \\
% = & \frac{\si^2_N}{N}
% \end{align*}
%
% So the expected value of the average of the sample is the average of the box no matter how
% large the sample.
%
% The variance of the sample average {\it decreases} as the sample size $N$ increases.
%
%
% \newpage
% \parbox[t]{0pt}{}
%
% \bigskip
%
% Example: $X_1,\ldots,X_N$ are draws from a box of 0's and 1's with replacement. Assume
% fraction of 1's in the box is $p$.
%
% \medskip
% Each $X_n$ is Ber(p), i.e. $EX_n=p, Var(X_n)=p(1-p)$.
%
% \medskip
% Since the draws are with replacement we can assume they are independent and
% so writing
% $S= \sum_{n=1}^N X_n$ we have that $S$ is Binomial(N,p).
%
% \bigskip
% $f_S(n) = \binom{N}{n} p^n (1-p)^{N-n}.$
%
% \bigskip
% We can compute $ES$ using the definition $\sum_{n=1}^N n \binom{N}{n} p^n (1-p)^{N-n}$
% but that requires some complicated algebra.
%
% \newpage
% Instead we use the rules for mean and variance of a sum:
%
% $E(S) = \sum_{n=1}^N EX_n = Np.$
%
% \bigskip
% And since $X_n$ are independent:
%
% $Var(S) = \sum_{n=1}^N Var(X_n) = Np(1-p).$
%
% \bigskip
% And for the sample average:
%
% $E \overline{X} = E \frac{S}{N} = \frac{1}{N} ES = p.$
%
% \bigskip
%
% $Var(\overline{X}) = Var(\frac{S}{N}) = \frac{1}{N^2} Np(1-p) = \frac{p(1-p)}{N}.$
%
% \medskip
% The expected value of the sample average is $p$ - the proportion of 1's in the box:
% it is centered in the `right' place.
%
% The variance of the sample average decreases with sample size.
%
% \end{frame}
%
% \begin{frame}[allowframebreaks, fragile]{Moment generating function}
%
% For any random variable $X$ and for any $t$ we write:
%
% \medskip
% $m_X(t) = E e^{tX} = \sum_{x \in R_X} e^{tx} f_X(x).$
%
% \medskip
% Note that the first derivative is:
%
% \medskip
% $m'_X(t) = \sum_{x \in R_X} x e^{tx} f_X(x),$ so that $m'_X(0) = EX.$
%
% \medskip
% The second derivative is:
%
% \medskip
% $m''_X(t) = \sum_{x \in R_X} x^2 e^{tx} f_X(x),$ so that $m''_X(0) = EX^2$
%
% \medskip
% The $r$'th derivative is
%
% \medskip
% $m^{(r)}_X(t) = \sum_{x \in R_X} x^r e^{tx} f_X(x),$ so that $m^{(r)}_X(0) = EX^r$.
%
% \medskip
% $m_X(t)$ is called the {\it moment generating function} of $X$.
%
% Moment generating functions are useful because of the following fact:
%
% {\it Two random variables with the same moment generating function have
% the same distribution.}
% \newpage
%
% \parbox[t]{0pt}{}
%
% \vskip 1in
%
% Examples:
%
% Bernoulli random variable:
%
% \bigskip
% $m(t) = ( e^{0 \cdot t}\cdot (1-p) + e^{1\cdot t} p) = 1-p+pe^t.$
%
% $EX=m'(0) = p, E(X^2)=m''(0) = p.$
%
% \newpage
% \bigskip
% Binomial random variable:
%
% \begin{align*}
% m(t) = & \sum_{n=1}^N e^{tn} \binom{N}{n} p^n (1-p)^{N-n} \\
% = &\sum_{n=1} \binom{N}{n} (pe^t)^n (1-p)^{N-n} \\
% (\mbox{Binomial formula}) \ = &  \ [pe^t + (1-p)]^N
% \end{align*}
%
% \medskip
% $m'(t) = N[pe^t + (1-p)]^{N-1} pe^t$
%
% \medskip
% $m''(t) = N(N-1)[pe^t + (1-p)]^{N-2}p^2e^{2t}+ N [pe^t + (1-p)]^{N-1}pe^t$.
%
% \medskip
% $EX = m'(0) = Np.$
%
% \medskip
% $E(X^2) = N(N-1)p^2 + Np = Np(1-p) + (Np)^2.$
%
% \medskip
% $Var(X) = E(X^2) - (EX)^2 = Np(1-p) + (Np)^2 - (Np)^2 = Np(1-p).$
%
% \end{frame}

\end{document}
